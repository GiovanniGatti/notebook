{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Problem\n",
    "\n",
    "Regression analysis is a set of statistical processes for estimating the relationships among variables. Formally,\n",
    "\n",
    "* The unknown parameters, denoted as $\\theta$ , which may represent a scalar or a vector.\n",
    "* The independent variables, $\\mathcal{X}$.\n",
    "* The dependent variable, $\\mathcal{Y}$.\n",
    "\n",
    "The goal is then to be able to predict $\\mathcal{Y}$ given < $\\mathcal{X}$, $\\theta$ > :\n",
    "\n",
    "$$\\mathcal{Y} \\approx h(X, \\theta)$$\n",
    "\n",
    "where $h(X, \\theta)$ is called the hypotesis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://drive.google.com/file/d/1cJHJ5AdcFd0tibQvCrME4ychIrvPQoGc/preview', width=340, height=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://drive.google.com/file/d/1WknHdpGr4HkJU3ZCuW9i0tBJDF6MDd5w/preview', width=340, height=220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Let's say that we decide to represent the hypothesis $h$ as a linear function of $\\mathcal{X}$:\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2$$\n",
    "\n",
    "Here, the $\\theta_i$’s are the parameters (also called weights) parameterizing the space of linear functions mapping from $\\mathcal{X}$ to $\\mathcal{Y}$. When there is no risk of confusion, we will drop the $\\theta$ subscript in $h_\\theta(x)$, and write it more simply as $h(x)$. To simplify our notation, we also introduce the convention of letting $x_0 = 1$ (the intercept term), so that\n",
    "\n",
    "$$h(x) = \\sum_{i=0}^{n} \\theta_i x_i = \\theta^T x$$\n",
    "\n",
    "In order to learn parameters $\\theta$ the most naive choice is to make $h(x)$ as close as possible from $\\mathcal{Y}$, which brings us to the cost function:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=0}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n",
    "\n",
    "Which measures the half total of the total square distance from model to reality.IFrame('https://drive.google.com/file/d/1WknHdpGr4HkJU3ZCuW9i0tBJDF6MDd5w/preview', width=340, height=220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Mean Aquares algorithm\n",
    "\n",
    "We want to choose $\\theta$ so as to minimize the cost function $J(\\theta)$. To do so, lets consider applying [gradient descent algorithm](/notebooks/math/gradient-descent.ipynb):\n",
    "\n",
    "$$\\theta_{j} := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$$\n",
    "\n",
    "(every single interation, we simultaneously update all values of $\\theta$)\n",
    "\n",
    "Here, $\\alpha$ is usually called the **learning rate**.\n",
    "\n",
    "Working out this partial derivative we get:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2} (h_\\theta(x) - y)^2\n",
    "\\\\= \\frac{1}{2} \\cdot 2 (h_\\theta(x) - y) \\cdot \\frac{\\partial}{\\partial \\theta_j} (h_\\theta(x) - y)\n",
    "\\\\= (h_\\theta(x) - y) \\cdot \\frac{\\partial}{\\partial \\theta_j} \\sum_{i=0}^{n} (\\theta_i x_i - y)\n",
    "\\\\= (h_\\theta(x) - y) x_j$$\n",
    "\n",
    "therefore we end up with:\n",
    "\n",
    "$$\\theta_{j} := \\theta_j + \\alpha \\cdot (y^{(i)}  - h_\\theta(x^{(i)})) x_j^{(i)}$$\n",
    "\n",
    "The rule is called the LMS update rule and is also known as the Widrow-Hoff learning rule. Note that the magnitude of the update is proportional to the error term $(y^{(i)}  - h_\\theta(x^{(i)}))$. This method looks at every example in the entire training set on every step, and is called **batch gradient descent**. It is also important that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus **gradient descent always converges** (assuming the learning rate $\\alpha$ is not too large) to the global minimum.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Let's say that we want to predict the selling price of a house. We indentified two independent variables that contribute to the selling price (living area and number of bedrooms) and we collected the following data: \n",
    "\n",
    "| Living area (square feet) | #bedrooms   | price (1000$s) |\n",
    "|---------------------------|-------------|----------------|\n",
    "| 2104                      | 3           | 400            |\n",
    "| 1600                      | 3           | 330            |\n",
    "| 2400                      | 3           | 369            |\n",
    "| 1416                      | 2           | 232            |\n",
    "| 3000                      | 4           | 540            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "t = np.array([[2104, 3, 400],\n",
    "             [1600, 3, 330],\n",
    "             [2400, 3, 369],\n",
    "             [1416, 2, 232],\n",
    "             [3000, 4, 540]])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(t[:,0], t[:,1], t[:,2])\n",
    "ax.set_xlabel('Living area (quare feet)')\n",
    "ax.set_ylabel('# Bedrooms')\n",
    "ax.set_zlabel('Selling Price (1000$)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying now LMS algorith:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "t = np.array([[1, 2104, 3, 400],\n",
    "             [1, 1600, 3, 330],\n",
    "             [1, 2400, 3, 369],\n",
    "             [1, 1416, 2, 232],\n",
    "             [1, 3000, 4, 540]])\n",
    "\n",
    "# 89.60, θ1 = 0.1392, θ2 = −8.738\n",
    "\n",
    "theta = np.array([89.0, 0.1, -8.0])\n",
    "theta_n = np.array([0, 0, 0])\n",
    "\n",
    "j = 0\n",
    "i = 1\n",
    "\n",
    "alpha = 0.01\n",
    "num_parameters = 3\n",
    "num_of_samples = 5\n",
    "\n",
    "# print (error)\n",
    "\n",
    "for x in range(0, 2):\n",
    "    for j in range(0, num_parameters):\n",
    "        partial = .0\n",
    "        for i in range(0, num_of_samples):\n",
    "            print(theta)\n",
    "            print (i)\n",
    "            print (j)\n",
    "            print (t[i,3])\n",
    "            print (t[i,0:4])\n",
    "            print (theta * t[i,0:3])\n",
    "            print (np.sum(theta * t[i,0:3]))\n",
    "            print (\"error \", t[i,3] - np.sum(theta * t[i,0:3]))\n",
    "            print (t[i,j])\n",
    "            partial = partial + (t[i,3] - np.sum(theta * t[i,0:3])) * t[i,j]\n",
    "            print (\"partial\", partial)\n",
    "            print(\"---\")\n",
    "        theta_n[j] = theta[j] + alpha * partial\n",
    "    theta = theta_n\n",
    "\n",
    "print (theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
