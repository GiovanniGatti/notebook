{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Problem\n",
    "\n",
    "Regression analysis is a set of statistical processes for estimating the relationships among variables. Formally,\n",
    "\n",
    "* The unknown parameters, denoted as $\\theta$ , which may represent a scalar or a vector.\n",
    "* The independent variables, $\\mathcal{X}$.\n",
    "* The dependent variable, $\\mathcal{Y}$.\n",
    "\n",
    "The goal is then to be able to predict $\\mathcal{Y}$ given < $\\mathcal{X}$, $\\theta$ > :\n",
    "\n",
    "$$\\mathcal{Y} \\approx h(X, \\theta)$$\n",
    "\n",
    "where $h(X, \\theta)$ is called the hypotesis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"340\"\n",
       "            height=\"220\"\n",
       "            src=\"https://drive.google.com/file/d/1cJHJ5AdcFd0tibQvCrME4ychIrvPQoGc/preview\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0790050cf8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://drive.google.com/file/d/1cJHJ5AdcFd0tibQvCrME4ychIrvPQoGc/preview', width=340, height=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"340\"\n",
       "            height=\"220\"\n",
       "            src=\"https://drive.google.com/file/d/1WknHdpGr4HkJU3ZCuW9i0tBJDF6MDd5w/preview\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f07900509b0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://drive.google.com/file/d/1WknHdpGr4HkJU3ZCuW9i0tBJDF6MDd5w/preview', width=340, height=220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Let's say that we decide to represent the hypothesis $h$ as a linear function of $\\mathcal{X}$:\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2$$\n",
    "\n",
    "Here, the $\\theta_i$’s are the parameters (also called weights) parameterizing the space of linear functions mapping from $\\mathcal{X}$ to $\\mathcal{Y}$. When there is no risk of confusion, we will drop the $\\theta$ subscript in $h_\\theta(x)$, and write it more simply as $h(x)$. To simplify our notation, we also introduce the convention of letting $x_0 = 1$ (the intercept term), so that\n",
    "\n",
    "$$h(x) = \\sum_{i=0}^{n} \\theta_i x_i = \\theta^T x$$\n",
    "\n",
    "In order to learn parameters $\\theta$ the most naive choice is to make $h(x)$ as close as possible from $\\mathcal{Y}$, which brings us to the cost function:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n",
    "\n",
    "Which measures the half total of the total square distance from model to reality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Mean Aquares algorithm\n",
    "\n",
    "We want to choose $\\theta$ so as to minimize the cost function $J(\\theta)$. To do so, lets consider applying [gradient descent algorithm](/notebooks/math/gradient-descent.ipynb):\n",
    "\n",
    "$$\\theta_{j} := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$$\n",
    "\n",
    "(every single interation, we simultaneously update all values of $\\theta$)\n",
    "\n",
    "Here, $\\alpha$ is usually called the **learning rate**.\n",
    "\n",
    "Working out this partial derivative we get:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2} (h_\\theta(x) - y)^2\n",
    "\\\\= \\frac{1}{2} \\cdot 2 (h_\\theta(x) - y) \\cdot \\frac{\\partial}{\\partial \\theta_j} (h_\\theta(x) - y)\n",
    "\\\\= (h_\\theta(x) - y) \\cdot \\frac{\\partial}{\\partial \\theta_j} \\sum_{i=0}^{n} (\\theta_i x_i - y)\n",
    "\\\\= (h_\\theta(x) - y) x_j$$\n",
    "\n",
    "therefore we end up with:\n",
    "\n",
    "$$\\theta_{j} := \\theta_j + \\alpha \\cdot (y^{(i)}  - h_\\theta(x^{(i)})) x_j^{(i)}$$\n",
    "\n",
    "This formula works with a single example for a single parameter. We can generalize too\n",
    "\n",
    "$\\text{repeat until convergence } \\big\\{$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\theta_{0} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta_{j}\n",
    "\\end{bmatrix}\n",
    ":=\n",
    "\\begin{bmatrix}\n",
    "    \\theta_{0} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta_{j}\n",
    "\\end{bmatrix}\n",
    "+ \\alpha \\cdot\n",
    "\\sum_{i=1}^{m}\n",
    "\\Bigg(y^{(i)} - \n",
    "\\begin{bmatrix}\n",
    "    x^{i}_{0} & \\dots & x^{i}_{j} \\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "    \\theta_{0} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta_{j}\n",
    "\\end{bmatrix}\n",
    "\\Bigg) \\cdot\n",
    "\\begin{bmatrix}\n",
    "    x^{i}_{0} \\\\ \n",
    "    \\dots \\\\\n",
    "    x^{i}_{j}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\big\\}$\n",
    "\n",
    "The rule is called the LMS update rule and is also known as the Widrow-Hoff learning rule. Note that the magnitude of the update is proportional to the error term $(y^{(i)}  - h_\\theta(x^{(i)}))$. This method looks at every example in the entire training set on every step, and is called **batch gradient descent**. It is also important that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus **gradient descent always converges** (assuming the learning rate $\\alpha$ is not too large) to the global minimum.\n",
    "\n",
    "Notice that $\\alpha$ has a huge part in the stability of the whole process. As bigger the dataset gets, smaller it has to be. The biggest is the precision we want, the smaller it has to be. Smaller it gets, the slower the convergence will be. \n",
    "\n",
    "#### Example\n",
    "\n",
    "Let's say that we want to predict the selling price of a house. We indentified two independent variables that contribute to the selling price (living area and number of bedrooms) and we collected the following data: \n",
    "\n",
    "| Living area (square feet) | #bedrooms   | price (1000$s) |\n",
    "|---------------------------|-------------|----------------|\n",
    "| 2104                      | 3           | 400            |\n",
    "| 1600                      | 3           | 330            |\n",
    "| 2400                      | 3           | 369            |\n",
    "| 1416                      | 2           | 232            |\n",
    "| 3000                      | 4           | 540            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "θ₀= [90.   0.4 -9. ]\n",
      "J(θ₀)= 1496423.12\n",
      "θ= [89.99988093  0.14968334 -9.00033479]\n",
      "J(θ)= 8142.323265782019\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1, 2104, 3],\n",
    "             [1, 1600, 3],\n",
    "             [1, 2400, 3],\n",
    "             [1, 1416, 2],\n",
    "             [1, 3000, 4]])\n",
    "\n",
    "y = np.array([400, 330, 369, 232, 540])\n",
    "\n",
    "theta = np.array([90, .4, -9.0])\n",
    "\n",
    "cost = float(.0)\n",
    "for i in range(0, np.size(x, 0)):\n",
    "    cost += (y[i] - np.sum(theta * x[i]))**2\n",
    "\n",
    "print(\"\\u03B8\\u2080=\", theta)\n",
    "print(\"J(\\u03B8\\u2080)=\", cost)\n",
    "\n",
    "alpha = 0.00000001\n",
    "last_cost = cost\n",
    "while True:\n",
    "    partial = np.zeros(theta.size)\n",
    "    for i in range(0, np.size(x, 0)):\n",
    "        partial += (y[i] - np.sum(x[i] * theta)) * x[i]\n",
    "    theta += alpha * partial\n",
    "    \n",
    "    cost = float(.0)\n",
    "    for i in range(0, np.size(x, 0)):\n",
    "        cost += (y[i] - np.sum(theta * x[i]))**2\n",
    "    \n",
    "    if(last_cost - cost < 0.0001):\n",
    "        last_cost = cost\n",
    "        break\n",
    "    last_cost = cost\n",
    "\n",
    "print(\"\\u03B8=\", theta)\n",
    "print(\"J(\\u03B8)=\", last_cost)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
