{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Problem\n",
    "\n",
    "Regression analysis is a set of statistical processes for estimating the relationships among variables. Formally,\n",
    "\n",
    "* The unknown parameters, denoted as $\\theta$ , which may represent a scalar or a vector.\n",
    "* The independent variables, $\\mathcal{X}$.\n",
    "* The dependent variable, $\\mathcal{Y}$.\n",
    "\n",
    "The goal is then to be able to predict $\\mathcal{Y}$ given < $\\mathcal{X}$, $\\theta$ > :\n",
    "\n",
    "$$\\mathcal{Y} \\approx h(X, \\theta)$$\n",
    "\n",
    "where $h(X, \\theta)$ is called the hypotesis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"340\"\n",
       "            height=\"220\"\n",
       "            src=\"https://drive.google.com/file/d/1cJHJ5AdcFd0tibQvCrME4ychIrvPQoGc/preview\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa348da47b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://drive.google.com/file/d/1cJHJ5AdcFd0tibQvCrME4ychIrvPQoGc/preview', width=340, height=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"340\"\n",
       "            height=\"220\"\n",
       "            src=\"https://drive.google.com/file/d/1WknHdpGr4HkJU3ZCuW9i0tBJDF6MDd5w/preview?t=575\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa348da46a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://drive.google.com/file/d/1WknHdpGr4HkJU3ZCuW9i0tBJDF6MDd5w/preview?t=575', width=340, height=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"340\"\n",
       "            height=\"220\"\n",
       "            src=\"https://drive.google.com/file/d/1kAlwLwUJiKjN8HyQlquw2Q3kzB1SWyRt/preview\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa348d747f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://drive.google.com/file/d/1kAlwLwUJiKjN8HyQlquw2Q3kzB1SWyRt/preview', width=340, height=220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Let's say that we decide to represent the hypothesis $h$ as a linear function of $\\mathcal{X}$:\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2$$\n",
    "\n",
    "Here, the $\\theta_i$’s are the parameters (also called weights) parameterizing the space of linear functions mapping from $\\mathcal{X}$ to $\\mathcal{Y}$. When there is no risk of confusion, we will drop the $\\theta$ subscript in $h_\\theta(x)$, and write it more simply as $h(x)$. To simplify our notation, we also introduce the convention of letting $x_0 = 1$ (the intercept term), so that\n",
    "\n",
    "$$h(x) = \\sum_{i=0}^{n} \\theta_i x_i = \\theta^T x$$\n",
    "\n",
    "In order to learn parameters $\\theta$ the most naive choice is to make $h(x)$ as close as possible from $\\mathcal{Y}$, which brings us to the cost function:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n",
    "\n",
    "Which measures the half total of the total square distance from model to reality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Mean Aquares algorithm\n",
    "\n",
    "We want to choose $\\theta$ so as to minimize the cost function $J(\\theta)$. To do so, lets consider applying [gradient descent algorithm](/notebooks/math/gradient-descent.ipynb):\n",
    "\n",
    "$$\\theta_{j} := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$$\n",
    "\n",
    "(every single interaction, we simultaneously update all values of $\\theta$)\n",
    "\n",
    "Here, $\\alpha$ is usually called the **learning rate**.\n",
    "\n",
    "Working out this partial derivative we get:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2} (h_\\theta(x) - y)^2\n",
    "\\\\= \\frac{1}{2} \\cdot 2 (h_\\theta(x) - y) \\cdot \\frac{\\partial}{\\partial \\theta_j} (h_\\theta(x) - y)\n",
    "\\\\= (h_\\theta(x) - y) \\cdot \\frac{\\partial}{\\partial \\theta_j} \\sum_{i=0}^{n} (\\theta_i x_i - y)\n",
    "\\\\= (h_\\theta(x) - y) x_j$$\n",
    "\n",
    "therefore we end up with:\n",
    "\n",
    "$$\\theta_{j} := \\theta_j + \\alpha \\cdot (y^{(i)}  - h_\\theta(x^{(i)})) x_j^{(i)}$$\n",
    "\n",
    "This formula works with a single example for a single parameter. We can generalize too\n",
    "\n",
    "$\\text{repeat until convergence } \\big\\{$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\theta_{0} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta_{j}\n",
    "\\end{bmatrix}\n",
    ":=\n",
    "\\begin{bmatrix}\n",
    "    \\theta_{0} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta_{j}\n",
    "\\end{bmatrix}\n",
    "+ \\alpha \\cdot\n",
    "\\sum_{i=1}^{m}\n",
    "\\Bigg(y^{(i)} - \n",
    "\\begin{bmatrix}\n",
    "    x^{i}_{0} & \\dots & x^{i}_{j} \\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "    \\theta_{0} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta_{j}\n",
    "\\end{bmatrix}\n",
    "\\Bigg) \\cdot\n",
    "\\begin{bmatrix}\n",
    "    x^{i}_{0} \\\\ \n",
    "    \\dots \\\\\n",
    "    x^{i}_{j}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\big\\}$\n",
    "\n",
    "The rule is called the LMS update rule and is also known as the Widrow-Hoff learning rule. Note that the magnitude of the update is proportional to the error term $(y^{(i)}  - h_\\theta(x^{(i)}))$. This method looks at every example in the entire training set on every step, and is called **batch gradient descent**. It is also important that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus **gradient descent always converges** (assuming the learning rate $\\alpha$ is not too large) to the global minimum.\n",
    "\n",
    "Notice that $\\alpha$ has a huge part in the stability of the whole process. As bigger the dataset gets, smaller it has to be. The biggest is the precision we want, the smaller it has to be. Smaller it gets, the slower the convergence will be. It is though common to run (stochastic) gradient descent as we have described it and with a fixed learning rate $\\alpha$, by slowly letting the learning rate $\\alpha$ decrease to zero as the algorithm runs, it is also possible to ensure that the parameters will converge to the global minimum rather then merely oscillate around the minimum.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Let's say that we want to predict the selling price of a house. We indentified two independent variables that contribute to the selling price (living area and number of bedrooms) and we collected the following data: \n",
    "\n",
    "| Living area (square feet) | #bedrooms   | price (1000$s) |\n",
    "|---------------------------|-------------|----------------|\n",
    "| 2104                      | 3           | 400            |\n",
    "| 1600                      | 3           | 330            |\n",
    "| 2400                      | 3           | 369            |\n",
    "| 1416                      | 2           | 232            |\n",
    "| 3000                      | 4           | 540            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "θ₀= [90.   0.4 -9. ]\n",
      "J(θ₀)= 1496423.12\n",
      "θ= [89.99988093  0.14968334 -9.00033479]\n",
      "J(θ)= 8142.323265782019\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1, 2104, 3],\n",
    "             [1, 1600, 3],\n",
    "             [1, 2400, 3],\n",
    "             [1, 1416, 2],\n",
    "             [1, 3000, 4]])\n",
    "\n",
    "y = np.array([400, 330, 369, 232, 540])\n",
    "\n",
    "theta = np.array([90, .4, -9.0])\n",
    "\n",
    "cost = float(.0)\n",
    "for i in range(0, np.size(x, 0)):\n",
    "    cost += (y[i] - np.sum(theta * x[i]))**2\n",
    "\n",
    "print(\"\\u03B8\\u2080=\", theta)\n",
    "print(\"J(\\u03B8\\u2080)=\", cost)\n",
    "\n",
    "alpha = 0.00000001\n",
    "last_cost = cost\n",
    "while True:\n",
    "    partial = np.zeros(theta.size)\n",
    "    for i in range(0, np.size(x, 0)):\n",
    "        partial += (y[i] - np.sum(x[i] * theta)) * x[i]\n",
    "    theta += alpha * partial\n",
    "    \n",
    "    cost = float(.0)\n",
    "    for i in range(0, np.size(x, 0)):\n",
    "        cost += (y[i] - np.sum(theta * x[i]))**2\n",
    "    \n",
    "    if(last_cost - cost < 0.0001):\n",
    "        last_cost = cost\n",
    "        break\n",
    "    last_cost = cost\n",
    "\n",
    "print(\"\\u03B8=\", theta)\n",
    "print(\"J(\\u03B8)=\", last_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equations\n",
    "\n",
    "Another way of minimizing $J(\\theta)$ without resorting to an iterative algorithm is by explicitly taking its derivatives with respect to $\\theta_{i}$'s and setting them to zero.\n",
    "\n",
    "Let \n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "    (x^{(1)})^T \\\\\n",
    "    (x^{(2)})^T \\\\\n",
    "    \\vdots \\\\\n",
    "    (x^{(m)})^T \n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "%\n",
    "\\quad\n",
    "\\vec{y} = \\begin{bmatrix}\n",
    "    (y^{(1)}) \\\\\n",
    "    (y^{(2)}) \\\\\n",
    "    \\vdots \\\\\n",
    "    (y^{(m)}) \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $X$ is the **design matrix** (m-by-n+1, if we include of the intercept term) that contains the training examples' input values and $\\vec{y}$ is the m-dimensional vector containing all target values from the training set.\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} \\big(h_{\\theta}(x^{(i)}) - y^{(i)}\\big)^2 = \\frac{1}{2} \\big(X \\theta - \\vec{y} \\big)^T \\big(X \\theta - \\vec{y} \\big)\n",
    "$$\n",
    "\n",
    "Finally, making use of [Matrix derivatives](/notebooks/notebook/math/matrix-derivatives.ipynb), we get\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\theta) = \\nabla_{\\theta} \\frac{1}{2} \\big( X \\theta - \\vec{y} \\big)^T \\big( X \\theta - \\vec{y} \\big)\n",
    "= X^T X \\theta - X^T \\vec{y}\n",
    "$$\n",
    "\n",
    "Please, refer to lecture notes (here attached) for math details.\n",
    "\n",
    "In order to minimize $J(\\theta)$, we set its derivatives to zero and we obtain the **normal equations**:\n",
    "\n",
    "$$\n",
    "X^T X \\theta = X^T \\vec{y} \\iff \\theta = (X^T X)^{-1} X^T \\vec{y}\n",
    "$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "Applying the normal equations to the last example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "θ= [-7.04346018e+01  6.38433756e-02  1.03436047e+02]\n",
      "J(θ)= 1444.144432699083\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1, 2104, 3],\n",
    "             [1, 1600, 3],\n",
    "             [1, 2400, 3],\n",
    "             [1, 1416, 2],\n",
    "             [1, 3000, 4]])\n",
    "\n",
    "y = np.array([400, 330, 369, 232, 540])\n",
    "\n",
    "theta = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(x), x)), np.transpose(x)), y)\n",
    "\n",
    "cost = .0\n",
    "for i in range(0, np.size(x, 0)):\n",
    "    cost += (y[i] - np.sum(theta * x[i]))**2\n",
    "\n",
    "print(\"\\u03B8=\", theta)\n",
    "print(\"J(\\u03B8)=\", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Interpretation\n",
    "\n",
    "When faced to a regression problem, why might linear regression, and specifically why might the least-squares cost function $J$, be a reasonable choice?\n",
    "\n",
    "Let's assume that the target variables and the inputs are related via the following equation:\n",
    "\n",
    "$$y^{(i)} = \\theta^T x^{(i)} + \\epsilon^{(i)}$$\n",
    "\n",
    "where $\\epsilon^{(i)}$ is an error term that captures either unmodeled effects or random noise. Let us further Let us further assume that the $\\epsilon^{(i)}$ are distributed independently and identically distributed according to a Gaussian distribution (also called a Normal distribution) with mean zero and some variance $\\sigma^2$. We can write this assumption as $\\epsilon^{(i)} \\sim \\mathcal{N}(\\mu,\\,\\sigma^{2})$.\n",
    "\n",
    "That implies\n",
    "\n",
    "$$p(\\epsilon^{(i)}) = \\frac{1}{\\sqrt{2 \\pi}\\sigma} exp \\big(- \\frac{(\\epsilon^{(i)})^2}{2 \\sigma^2} \\big)\n",
    "\\\\\\iff\n",
    "p(y^{(i)} | x^{(i)}; \\theta) = \\frac{1}{\\sqrt{2 \\pi}\\sigma} exp \\big(- \\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2 \\sigma^2} \\big)\n",
    "$$\n",
    "\n",
    "The above formula gives the distribution of $y^{(i)}$ given $x^{(i)}$ and parameterized by $\\theta$. Considering now the entire dataset we can say that the probability of data is given by $p(\\vec{y} | X; \\theta)$. If we whish to explicitly view it as a function of $\\theta$, we end up with the likelyhood function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\theta) &= L(\\theta; X, \\vec{y}) \\\\\n",
    "&= p(\\vec{y}|X;\\theta) \\\\\n",
    "&= \\prod_{i=1}^{m} p(y^{(i)}|x^{(i)}; \\theta) \\\\\n",
    "&= \\prod_{i=1}^{m} \\frac{1}{\\sqrt{2 \\pi}\\sigma} exp \\big(- \\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2 \\sigma^2} \\big)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "One trick is, instead of maximizing the likelyhood function, to maximize its **log**.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "l(\\theta) &= \\log L(\\theta) \\\\\n",
    "&= ... \\\\\n",
    "&= m \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma} - \\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{m} \\big( y^{(i)} - \\theta^T x^{(i)} \\big)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence, maximizing $l(\\theta)$ gives the same answer as minimizing\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i=1}^{m} \\big( y^{(i)} - \\theta^T x^{(i)} \\big)^2\n",
    "$$\n",
    "\n",
    "which we recognize to be $J(\\theta)$, our original least-squares cost function.\n",
    "\n",
    "Note that our final choice of $\\theta$ did not depend on what was $\\sigma^2$, and indeed we'd have arrived at the same result even if $\\sigma^2$ were unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally Weighted Regression\n",
    "\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa348ce4dd8>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([0.5, 1.3, 3.0, 3.7, 4.0, 4.4])\n",
    "y = np.array([3.5, 5.6, 6.6, 6.0, 5.7, 5.6])\n",
    "\n",
    "pylab.plot(x, y, 'bo')\n",
    "\n",
    "## to design matrix (adding intercept term)\n",
    "X = np.insert(x.reshape(x.size, 1), 0, 1, axis=1)\n",
    "\n",
    "theta = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X), X)), np.transpose(X)), y)\n",
    "lx = np.linspace(0.5, 4.5, 100)\n",
    "ly = theta[0] + theta[1] * lx\n",
    "pylab.plot(lx, ly, 'r')\n",
    "\n",
    "## to design matrix (adding x^2 term)\n",
    "X = np.concatenate((np.ones(x.size).reshape(x.size, 1), x.reshape(x.size, 1), x.reshape(x.size, 1)**2), axis=1)\n",
    "theta = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X), X)), np.transpose(X)), y)\n",
    "ly = theta[0] + theta[1] * lx + theta[2] * lx**2\n",
    "pylab.plot(lx, ly, 'orange')\n",
    "\n",
    "## to design matrix (adding up to x^5 term)\n",
    "X = np.concatenate(\n",
    "    (np.ones(x.size).reshape(x.size, 1),\n",
    "     x.reshape(x.size, 1),\n",
    "     x.reshape(x.size, 1)**2,\n",
    "     x.reshape(x.size, 1)**3,\n",
    "     x.reshape(x.size, 1)**4,\n",
    "     x.reshape(x.size, 1)**5), axis=1)\n",
    "theta = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X), X)), np.transpose(X)), y)\n",
    "ly = theta[0] + theta[1] * lx + theta[2] * lx**2 + theta[3] * lx**3 + theta[4] * lx**4 + theta[5] * lx**5\n",
    "pylab.plot(lx, ly, 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red line on the above figure shows the result of fitting a $y = \\theta_{0} + \\theta_{1}x$ to a dataset. We see that the data doesn't really lie on straight line, and so the fit is not very good. Instead, if we artificially add an extra feature $x^2$, and fit $y = \\theta_{0} + \\theta_{1}x + \\theta_{2}x^2$,then we obtain a slightly better fit to the data (orange line). Naively, it might seem that the more features we add, the better. However, there is also a danger in adding too many features: the green line is the result of fitting a 5-th order polynomial. We see that even though the fitted curve passes through the data perfectly, we would not expect this to be a very good predictor. Without formally defining what these terms mean, we'll say the red line shows an instance of **underfitting** — in which the data clearly shows structure not captured by the model — and the figure on the right is an example of **overfitting**.\n",
    "\n",
    "The locally weighted linear regression algorithm does the following: fit $\\theta$ to minimize $\\sum_{i} \\omega^{(i)} \\big(y^{(i)} - \\theta^T x^{(i)} \\big)^2$, where $\\omega^{(i)}$'s are non-negative valued **weights**. A fairly standard choice for weights is\n",
    "\n",
    "$$w^{(i)} = exp(- \\frac{(x^{(i)} - x)^2}{2 \\tau^2})$$\n",
    "\n",
    "Note that the weights depend on the particular point $x$ at which we're trying to evaluate $x$. $\\theta$ is chosen giving a much higher \"weight\" to the (errors on) training examples close to the query point $x$. The parameter\n",
    "$\\tau$ controls how quickly the weight of a training example falls off with distance of its $x^{(i)}$ from the query point $x$; $\\tau$ is called the **bandwidth** parameter.\n",
    "\n",
    "Locally weighted linear regression is an example of a **non-parametric** algorithm. The (unweighted) linear regression algorithm is known as a parametric learning algorithm, because it has a fixed, finite number of parameters (the $\\theta_{i}$'s), which are fit to the data. Once we've fit the $\\theta_{i}$'s and stored them away, we no longer need to\n",
    "keep the training data around to make future predictions. In contrast, to make predictions using locally weighted linear regression, we need to keep the entire training set around. The term \"non-parametric\" (roughly) refers to the fact that the amount of stuff we need to keep in order to represent the hypothesis h grows linearly with the size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
