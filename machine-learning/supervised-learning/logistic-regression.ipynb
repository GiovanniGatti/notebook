{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Logistic Regression\n",
    "\n",
    "The classification problem is much like the [regression problem](/notebooks/machine-learning/supervised-learning/linear-regression.ipynb), except that values we want to predict $\\mathcal{y}$ take only a small number of discrete values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Using the [linear regression](/notebooks/machine-learning/supervised-learning/linear-regression.ipynb) approach to predict $\\mathcal{y}$ given $\\mathcal{x}$ may perform very poorly. The reason is explicitly shown in the below example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "## Learning first round\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "x = np.array([2, 3, 4, 5, 6, 7])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "reg.fit(x.reshape((x.size,1)), y)\n",
    "lx = np.linspace(1, 8, 10)\n",
    "ly = reg.intercept_ + reg.coef_ * lx\n",
    "\n",
    "pylab.plot(lx, ly, 'blue')\n",
    "pylab.plot(x, y, 'bo')\n",
    "\n",
    "## Learning second round\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "x = np.array([2, 3, 4, 5, 6, 7, 15])\n",
    "y = np.array([0, 0, 0, 1, 1, 1, 1])\n",
    "\n",
    "reg.fit(x.reshape((x.size,1)), y)\n",
    "lx = np.linspace(1, 15, 10)\n",
    "ly = reg.intercept_ + reg.coef_ * lx\n",
    "\n",
    "pylab.plot(lx, ly, 'g')\n",
    "pylab.plot([15], 1, 'go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we are performing linear regression over a training set containing 6 entries such that:\n",
    "\n",
    "| y | 0 | 0 | 0 | 1 | 1 | 1 |\n",
    "|:-:|---|---|---|---|---|---|\n",
    "| x | 2 | 3 | 4 | 5 | 6 | 7 |\n",
    "\n",
    "The output generated will be the blue line, which is essentially a good predictor. However, if we add a new point to our table, let's say $(15, 1)$, which shouldn't change our modeling, but it will affect our model changing its predictions (see green line).\n",
    "\n",
    "To fix this, lets change the form for our hypotheses $h_{\\theta}(x)$. We will choose\n",
    "\n",
    "$$h_{\\theta} = g(\\theta^{T}x) = \\frac{1}{1+e^{-\\theta^{T}x}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$g(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "is called the logistic function or the sigmoid function.\n",
    "\n",
    "Other functions that smoothly increase from 0 to 1 can also be used, but for good reasons, the sigmoid function is a fairly good natural choice. Before moving on, here's a useful property of the derivative of the sigmoid function, which we write a $g'$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g'(z) &= \\frac{\\partial}{\\partial{z}} \\frac{1}{1+ e^{-z}} \\\\\n",
    "&= \\frac{1}{(1+e^{-z})^2} e^{-z} \\\\\n",
    "&= \\frac{1}{1+e^{-z}} \\big(1- \\frac{1}{1+e^{-z}}\\big) \\\\\n",
    "&= g(z)(1-g(z)) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So, given the logistic regression model, how do we fit $\\theta$ for it?\n",
    "\n",
    "Let's assume that\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\begin{align}\n",
    "P(y=1|x;\\theta) &= h_{\\theta}(x) \\\\\n",
    "P(y=0|x;\\theta) &= 1- h_{\\theta}(x) \\\\\n",
    "\\end{align}\n",
    "\\end{cases}\n",
    "\\implies p(y|x;\\theta)=(h_{\\theta}(x))^y (1-h_{\\theta}(x))^{1-y}\n",
    "$$\n",
    "\n",
    "Assuming that the m training samples were generated independently, we can maximize the log likelyhood:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\theta) &= p(\\vec{y} | X; \\theta) \\\\\n",
    "&= \\prod_{i=1}^{m} p(y^{(i)} | x^{(i)}; \\theta) \\\\\n",
    "&= \\prod_{i=1}^{m} (h_{\\theta}(x^{(i)}))^{y^{(i)}} (1 - h_{\\theta}(x^{(i)})^{1-y^{(i)}} \\\\\n",
    " \\\\\n",
    "l(\\theta) &= \\log L(\\theta) \\\\\n",
    "&= \\sum_{i=1}^{m} \\big( y^{(i)} \\log h(x^{(i)}) + (1 - y^{(i)}) \\log(1 - h(x^{(i)})) \\big)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "How to maximize the likelyhood? Using [gradient ascent](/notebooks/math/gradient-descent.ipynb) we surprisingly end up with the same update rule:\n",
    "\n",
    "$$\n",
    "\\theta_{j} := \\theta_{j} + \\alpha \\big(y^{(i)} - h_{\\theta}(x^{(i)})\\big) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "Although this looks like the same as [LMS algorithm](/notebooks/machine-learning/supervised-learning/linear-regression.ipynb), it is not the same because $h_{\\theta}(x^{(i)}$ is now defined as a non-linear function of $\\theta^T x^{(i)}$. Nonetheless, this is not a coincidence.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's implement locally-weighted version of the logistic regression. In the datafile [logistic-regression-data.tar.gz](https://drive.google.com/open?id=1ZfyFgzkZYxtA_O5zv4Y7oEIBAdlE4p1N) we have a group of points (x, y) that we want to learn from new predictions. The problem is to maximize:\n",
    "\n",
    "$$\n",
    "l(\\theta) = - \\frac{\\lambda}{2}\\theta^{T}\\theta + \\sum_{i=1}^{m} \\bigg( y^{(i)} \\log h_{\\theta}(x^{(i)}) + (1-y^{(i)}) \\log (1-h_{\\theta}(x^{(i)}))  \\bigg)\n",
    "$$\n",
    "\n",
    "The $- \\frac{\\lambda}{2}\\theta^{T}\\theta$ is known as the regularization parameter which is needed for Newton's method to perform well on this task. Here we will use $\\lambda = 0.0001$.\n",
    "\n",
    "Using this definition, the gradient of $l(\\theta)$ is given by\n",
    "\n",
    "$$\\nabla_{\\theta} l(\\theta) = X^{T}z - \\lambda \\theta$$\n",
    "\n",
    "where $z \\in \\mathbb{R}^{m}$ is defined by\n",
    "\n",
    "$$z_{i} = w^{(i)} (y^{(i)} - h_{\\theta}(x^{(i)}))$$\n",
    "\n",
    "And the Hessian is given by\n",
    "\n",
    "$$H = X^T D X - \\lambda I$$\n",
    "\n",
    "where $D \\in \\mathbb{R}^{m \\times m}$ is the diagonal matrix with \n",
    "\n",
    "$$D_{ii} = - w^{(i)} h_{\\theta}(x^{(i)})(1-h_{\\theta}(x^{(i)}))$$\n",
    "\n",
    "Given a query point $x$, we can choose to compute weights \n",
    "\n",
    "$$w^{(i)} = \\exp \\Big(- \\frac{\\Vert{x-x^{(i)}\\Vert}^2}{2\\tau^2} \\Big) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1ZfyFgzkZYxtA_O5zv4Y7oEIBAdlE4p1N into /tmp/tmp61nwb_7h/logistic-regression-data.tar.gz... Done.\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "%run logistic-regression-functions.py\n",
    "\n",
    "def lwlr(X, y, x, tau):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    w = np.zeros(m)\n",
    "    \n",
    "    for i in range(0, m):\n",
    "        w[i] = np.exp(- np.linalg.norm(x - X[i], ord=1)**2 / 2 * tau ** 2)\n",
    "    \n",
    "    z = np.ones(n)\n",
    "    while(np.linalg.norm(z, ord=1) > 0.000001):\n",
    "        h = 1 / (1 + np.exp(np.dot(-X, theta)))\n",
    "        print(h)\n",
    "        break\n",
    "\n",
    "X, y = load_dataset()\n",
    "\n",
    "lwlr(X, y, 0.15, 5)\n",
    "\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: - finish example\n",
    "      - stochastic gradient descent\n",
    "      - newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "310px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
