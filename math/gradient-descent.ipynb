{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. It can be understand as a \"batch\" processing because it requires, at each step, to go through all the data set to compute the gradient. In order find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.\n",
    "\n",
    "$$\\Theta_{i+1} := \\Theta_i - \\alpha \\frac{\\partial J(\\Theta)}{\\partial \\Theta_i}$$\n",
    "\n",
    "where \n",
    "\n",
    " * $\\Theta$ is known as the set of parameters\n",
    " * $\\alpha$ is the learning rate (which is usually a parameter of the learning algorithm\n",
    " * $J(\\Theta)$ is the cost function (the function of $\\Theta$ we are trying to optmize\n",
    "\n",
    "<img src=\"gradient-descent.png\" width=500px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Suppose that we want to find the local minimum of the function\n",
    "\n",
    "$$y = (x + 5)^2$$\n",
    "\n",
    "Then the algorithm can be represented as follows:\n",
    "\n",
    "$$x_{i+1} := x_i - \\alpha \\frac{\\partial f(x)}{\\partial x_i}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\frac{\\partial f(x)}{\\partial x} = \\frac{\\partial ((x + 5)^2)}{\\partial x} = 2 \\cdot (x + 5) \\cdot \\frac{\\partial (x + 5)}{\\partial x} = 2x + 10$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = (x + 5)**2\n",
    "\n",
    "pylab.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_i = 3 # The algorithm starts at x=3\n",
    "rate = 0.1 # Learning rate\n",
    "precision = 0.001 #This tells us when to stop the algorithm\n",
    "previous_step_size = 1\n",
    "df = lambda x: 2*(x+5) # gradient of our function\n",
    "\n",
    "while previous_step_size > precision: # stops when step size is smaller than desired precision\n",
    "    prev_x = x_i\n",
    "    x_i = x_i - rate * df(prev_x) # grad descent\n",
    "    previous_step_size = abs(x_i - prev_x)\n",
    "    \n",
    "print(\"The local minimum occurs at\", x_i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
